{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2109d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import scipy.sparse as sps\n",
    "import scipy.spatial.distance as dist\n",
    "\n",
    "A = jnp.linspace(0, 2, 2)\n",
    "print(A)\n",
    "\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "sys.path.insert(1, \"/your path here/\")\n",
    "import MB_Model\n",
    "import VPM as VPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdfb11",
   "metadata": {},
   "source": [
    "First we will compute the committor for the Muller-Brown potential using a 2-D subspace iteration.\n",
    "We first sample the short trajectories, then compute the grid-based reference solution for comparison, and then\n",
    "we train the subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648fb3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for short trajectories\n",
    "tau = 1500\n",
    "tau_net = tau\n",
    "stride = 1\n",
    "N = 50000\n",
    "dt = 0.001\n",
    "beta = 2.0\n",
    "n_mem = 2\n",
    "Delta = int((tau) / (n_mem + 2) - 1)\n",
    "Deep = True\n",
    "\n",
    "# generate data by integration\n",
    "Xs = MB_Model.Sample_Starts(N, vmax=12)\n",
    "Integrator_Params = [dt, beta, stride, tau + 1, Deep]\n",
    "ps = []\n",
    "for x in Xs:\n",
    "    ps.append([x] + Integrator_Params)\n",
    "Trajs = MB_Model.Integrator(ps)\n",
    "Data = []\n",
    "for t in Trajs:\n",
    "    Data.append(np.asarray(t[0 : max(tau, tau_net) + 1]))\n",
    "Data = np.asarray(Data)[:, :, 0:2]\n",
    "\n",
    "# process data\n",
    "ntraj = Data.shape[0]\n",
    "lentraj = Data.shape[1]\n",
    "InA = MB_Model.ellipseA(Data)\n",
    "InB = MB_Model.ellipseB(Data)\n",
    "Dts = VPM.MakeStopTimes(1 - InB - InA, tau)\n",
    "RHS = np.zeros((Data.shape[0], n_mem + 3))\n",
    "Data_stop = np.zeros((Data.shape[0], n_mem + 3, Data.shape[-1]))\n",
    "InA_stop = np.zeros((Data.shape[0], n_mem + 3))\n",
    "InB_stop = np.zeros((Data.shape[0], n_mem + 3))\n",
    "Data_stop[:, 0] = Data[:, 0]\n",
    "InA_stop[:, 0] = InA[:, 0]\n",
    "InB_stop[:, 0] = InB[:, 0]\n",
    "for i in range(1, n_mem + 2):\n",
    "    Data_stop[:, i] = Data[np.arange(len(Data)), np.minimum(Dts[:, 0], i * Delta)]\n",
    "    InA_stop[:, i] = InA[np.arange(len(Data)), np.minimum(Dts[:, 0], i * Delta)]\n",
    "    InB_stop[:, i] = InB[np.arange(len(Data)), np.minimum(Dts[:, 0], i * Delta)]\n",
    "    RHS[:, i] = 0 * np.minimum(Dts[:, 0], i * Delta)  # *(1-InB_stop[:,0])\n",
    "\n",
    "Data_stop[:, -1] = Data[np.arange(len(Data)), np.minimum(Dts[:, 0], tau_net)]\n",
    "InA_stop[:, -1] = InA[np.arange(len(Data)), np.minimum(Dts[:, 0], tau_net)]\n",
    "InB_stop[:, -1] = InB[np.arange(len(Data)), np.minimum(Dts[:, 0], tau_net)]\n",
    "RHS[:, -1] = 0 * np.minimum(Dts[:, 0], tau_net)  # *(1-InB_stop[:,0])\n",
    "InD_stop = InB_stop + InA_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0192a506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def Identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "# reference using grid\n",
    "QRef, DGrid_Reshape, InA_Reshape, InB_Reshape, L = MB_Model.Ref_Q(\n",
    "    MB_Model.V,\n",
    "    MB_Model.ellipseA,\n",
    "    MB_Model.ellipseB,\n",
    "    beta=2.0,\n",
    "    res=100,\n",
    "    xrange=[-1.5, 1],\n",
    "    yrange=[-0.5, 2],\n",
    "    Deep=True,\n",
    ")\n",
    "\n",
    "DGrid = DGrid_Reshape[:, None, :]\n",
    "InAGrid = InA_Reshape[:, None]\n",
    "InBGrid = InB_Reshape[:, None]\n",
    "InDGrid = InAGrid + InBGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3156d683",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 2  # subspace dimension\n",
    "Nets_List = [VPM.FF_ZBC, VPM.FF_Forecast_Subspace]\n",
    "LossdJac = VPM.Loss_Jacobi_Sigd\n",
    "Lossd = VPM.Loss_Subspace_Forecastd\n",
    "dphi_star_List = [\n",
    "    sigmoid,\n",
    "    Identity,\n",
    "]\n",
    "# We will use the softplus loss for the richardson iterate, so the first net\n",
    "# gets passed through a sigmoid to give the result.\n",
    "\n",
    "phi_t = np.concatenate(\n",
    "    [InB_stop[:, :, None], 1 - InD_stop[:, :, None]], axis=-1\n",
    ")  # Initial guess for subspace\n",
    "layer_sizes0 = [2, 64, 64, 64, 1]\n",
    "layer_sizes1 = [2, 64, 64, 64, k - 1]\n",
    "params_List = []\n",
    "params = VPM.init_network_params(layer_sizes0, jax.random.PRNGKey(0), scale=0.01)\n",
    "params_List.append(params)\n",
    "\n",
    "params = VPM.init_network_params(layer_sizes1, jax.random.PRNGKey(0), scale=0.01)\n",
    "params.append(np.zeros(k - 1))\n",
    "params.append(np.eye(k))\n",
    "params_List.append(params)\n",
    "\n",
    "guess = np.zeros_like(phi_t)\n",
    "guess[:, :, 0] = (\n",
    "    -50 * InA_stop + 50 * InB_stop\n",
    ")  # sigmoid of this functions satisfies (nearly) the committor boundary conditions\n",
    "InD_stop = InA_stop + InB_stop\n",
    "phi_t = np.ones((Data_stop.shape[0], Data_stop.shape[1], k))\n",
    "phi_t[:, :, 1] = phi_t[..., 1] * (1 - InD_stop)\n",
    "phi_t[:, :, 0] = InB_stop\n",
    "guessGrid = np.zeros((DGrid.shape[0], 1, k))\n",
    "guessGrid[:, :, 0] = -50 * InAGrid + 50 * InBGrid\n",
    "\n",
    "optimizer_List = [optax.adam(learning_rate=1e-3) for i in range(k)]\n",
    "opt_state_List = [optimizer_List[i].init(params_List[i]) for i in range(k)]\n",
    "\n",
    "# training loop\n",
    "for i in range(4):\n",
    "    alpha = 1 / (i + 1) ** 0.5\n",
    "    params_List, phi_t, opt_state_List, R = VPM.Subspace_Iteration_Forecast(\n",
    "        params_List,\n",
    "        Data_stop,\n",
    "        InD_stop,\n",
    "        phi_t,\n",
    "        guess,\n",
    "        RHS,\n",
    "        optimizer_List,\n",
    "        opt_state_List,\n",
    "        Inner_Iter=5000,\n",
    "        alpha=alpha,\n",
    "        BS=2000,\n",
    "        ls=None,\n",
    "        l2=0.0,\n",
    "        Orthogonalize=False,\n",
    "        Nets_List=Nets_List,\n",
    "        LossdJac=LossdJac,\n",
    "        Lossd=Lossd,\n",
    "        dphi_star_List=dphi_star_List,\n",
    "        Update_K=0,\n",
    "        Basis=False,\n",
    "        Mem=False,\n",
    "        n_QR_skip=0,\n",
    "        Train_Frac=1.0,\n",
    "    )\n",
    "\n",
    "    vec_t = VPM.Mem_Forecast(\n",
    "        phi_t[:, 0:-1, 1:] * (1 - InD_stop)[:, 0:-1][:, :, None], RHS[:, 0:-1], phi_t[:, :-1, 0]\n",
    "    )\n",
    "\n",
    "    vec_t = np.concatenate([[1] + list(vec_t)])\n",
    "    F = np.concatenate(\n",
    "        [\n",
    "            dphi_star_List[0](Nets_List[0](DGrid, InDGrid, params_List[0]) + guessGrid[:, :, 0])[\n",
    "                :, :, None\n",
    "            ],\n",
    "            dphi_star_List[1](Nets_List[1](DGrid, InDGrid, params_List[1])),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    Qgrid = F @ vec_t\n",
    "    plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=F[:, :, 0], cmap=\"bwr\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(vec_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fecc5f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=np.clip(Qgrid[:, 0], 0, 1), cmap=\"bwr\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=QRef, cmap=\"bwr\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece42c35",
   "metadata": {},
   "source": [
    "Here is the same workflow for the Muller-Brown MFPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8af03e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def Identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "# reference\n",
    "MFPTref, DGrid_Reshape, InA_Reshape, InB_Reshape, L = MB_Model.Ref_MFPT(\n",
    "    MB_Model.V,\n",
    "    MB_Model.ellipseB,\n",
    "    MB_Model.ellipseB,\n",
    "    beta=1.5,\n",
    "    res=100,\n",
    "    xrange=[-1.5, 1],\n",
    "    yrange=[-0.5, 2],\n",
    "    Deep=True,\n",
    ")\n",
    "DGrid = DGrid_Reshape[:, None, :]\n",
    "InAGrid = InA_Reshape[:, None]\n",
    "InBGrid = InB_Reshape[:, None]\n",
    "InDGrid = InBGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4be0dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tau = 1000\n",
    "tau_net = tau\n",
    "stride = 1\n",
    "\n",
    "N = 50000\n",
    "dt = 0.001\n",
    "\n",
    "# We will turn down beta to make the problem a bit easier.\n",
    "beta = 1.5\n",
    "n_mem = 2\n",
    "Delta = int((tau) / (n_mem + 2) - 1)\n",
    "\n",
    "Deep = True\n",
    "\n",
    "Xs = MB_Model.Sample_Starts(N, vmax=12)\n",
    "\n",
    "Integrator_Params = [dt, beta, stride, tau + 1, Deep]\n",
    "ps = []\n",
    "for x in Xs:\n",
    "    ps.append([x] + Integrator_Params)\n",
    "Trajs = MB_Model.Integrator(ps)\n",
    "Data = []\n",
    "for t in Trajs:\n",
    "    Data.append(np.asarray(t[0 : max(tau, tau_net) + 1]))\n",
    "Data = np.asarray(Data)[:, :, 0:2]\n",
    "\n",
    "ntraj = Data.shape[0]\n",
    "lentraj = Data.shape[1]\n",
    "InA = MB_Model.ellipseA(Data)\n",
    "InB = MB_Model.ellipseB(Data)\n",
    "Dts = VPM.MakeStopTimes(1 - InB, tau)\n",
    "RHS = np.zeros((Data.shape[0], n_mem + 3))\n",
    "Data_stop = np.zeros((Data.shape[0], n_mem + 3, Data.shape[-1]))\n",
    "InA_stop = np.zeros((Data.shape[0], n_mem + 3))\n",
    "InB_stop = np.zeros((Data.shape[0], n_mem + 3))\n",
    "Data_stop[:, 0] = Data[:, 0]\n",
    "InA_stop[:, 0] = InA[:, 0]\n",
    "InB_stop[:, 0] = InB[:, 0]\n",
    "for i in range(1, n_mem + 2):\n",
    "    Data_stop[:, i] = Data[np.arange(len(Data)), np.minimum(Dts[:, 0], i * Delta)]\n",
    "    InA_stop[:, i] = InA[np.arange(len(Data)), np.minimum(Dts[:, 0], i * Delta)]\n",
    "    InB_stop[:, i] = InB[np.arange(len(Data)), np.minimum(Dts[:, 0], i * Delta)]\n",
    "    RHS[:, i] = dt * np.minimum(Dts[:, 0], i * Delta)\n",
    "\n",
    "Data_stop[:, -1] = Data[np.arange(len(Data)), np.minimum(Dts[:, 0], tau_net)]\n",
    "InA_stop[:, -1] = InA[np.arange(len(Data)), np.minimum(Dts[:, 0], tau_net)]\n",
    "InB_stop[:, -1] = InB[np.arange(len(Data)), np.minimum(Dts[:, 0], tau_net)]\n",
    "RHS[:, -1] = dt * np.minimum(Dts[:, 0], tau_net)\n",
    "InD_stop = InB_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be06cb5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "Nets_List = [VPM.FF_ZBC, VPM.FF_Forecast_Subspace]\n",
    "LossdJac = VPM.Loss_Jacobid\n",
    "Lossd = VPM.Loss_Subspace_Forecastd\n",
    "dphi_star_List = [Identity, Identity]\n",
    "phi_t = np.concatenate(\n",
    "    [InB_stop[:, :, None], 1 - InD_stop[:, :, None]], axis=-1\n",
    ")  # Initial guess for subspace\n",
    "\n",
    "layer_sizes0 = [2, 64, 64, 64, 1]  # Neural network architecture\n",
    "layer_sizes1 = [2, 64, 64, 64, k - 1]\n",
    "params_List = []\n",
    "params = VPM.init_network_params(layer_sizes0, jax.random.PRNGKey(0), scale=0.01)\n",
    "params_List.append(params)\n",
    "\n",
    "params = VPM.init_network_params(layer_sizes1, jax.random.PRNGKey(0), scale=0.01)\n",
    "params.append(np.zeros(k - 1))\n",
    "params.append(np.eye(k))\n",
    "params_List.append(params)\n",
    "\n",
    "guess = np.zeros_like(phi_t)\n",
    "guessGrid = np.zeros((DGrid.shape[0], 1, k))\n",
    "\n",
    "\n",
    "optimizer_List = [optax.adam(learning_rate=1e-3) for i in range(2)]\n",
    "opt_state_List = [optimizer_List[i].init(params_List[i]) for i in range(2)]\n",
    "\n",
    "# main training loop\n",
    "for i in range(30):\n",
    "    params_List, phi_t, opt_state_List, R = VPM.Subspace_Iteration_Forecast(\n",
    "        params_List,\n",
    "        Data_stop,\n",
    "        InD_stop,\n",
    "        phi_t,\n",
    "        guess,\n",
    "        RHS,\n",
    "        optimizer_List,\n",
    "        opt_state_List,\n",
    "        Inner_Iter=5000,\n",
    "        alpha=1.0,\n",
    "        BS=2000,\n",
    "        ls=None,\n",
    "        l2=1,\n",
    "        Orthogonalize=True,\n",
    "        Nets_List=Nets_List,\n",
    "        LossdJac=LossdJac,\n",
    "        Lossd=Lossd,\n",
    "        dphi_star_List=dphi_star_List,\n",
    "        Update_K=1,\n",
    "        Basis=False,\n",
    "        Mem=False,\n",
    "        n_QR_skip=0,\n",
    "        Train_Frac=1.0,\n",
    "    )\n",
    "\n",
    "    vec_t = VPM.Mem_Forecast(\n",
    "        phi_t[:, 0:-1, 0:] * (1 - InD_stop)[:, 0:-1][:, :, None], RHS[:, 0:-1], 0 * phi_t[:, :-1, 0]\n",
    "    )\n",
    "    F = np.concatenate(\n",
    "        [\n",
    "            dphi_star_List[0](Nets_List[0](DGrid, InDGrid, params_List[0]) + guessGrid[:, :, 0])[\n",
    "                :, :, None\n",
    "            ],\n",
    "            dphi_star_List[1](Nets_List[1](DGrid, InDGrid, params_List[1])),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    # we orthogonalize for the zero BC problem here, so we need to multiply by R^-1\n",
    "    Qgrid = F @ np.linalg.inv(R) @ vec_t\n",
    "    plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=Qgrid[:, 0], cmap=\"bwr\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(vec_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a76628f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Becs = F @ np.linalg.inv(R)\n",
    "plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=Qgrid[:, 0], cmap=\"bwr\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=MFPTref, cmap=\"bwr\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc0d8c",
   "metadata": {},
   "source": [
    "And finally the workflow for the Muller-Brown invariant subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ea8fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Initialize(Data, k):\n",
    "    phi_t = np.concatenate(\n",
    "        [(Data @ np.random.randn(Data.shape[-1]))[:, :, None] for i in range(k)], axis=-1\n",
    "    )\n",
    "    phi_t[:, :, 0] = 1\n",
    "    Q, R = np.linalg.qr(phi_t[:, 0])\n",
    "    R = R / len(phi_t)\n",
    "    return phi_t @ np.linalg.inv(R)\n",
    "\n",
    "\n",
    "def Subspace_Distance(X, Y):\n",
    "    P_x = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "    P_y = Y @ np.linalg.inv(Y.T @ Y) @ Y.T\n",
    "    return np.real(np.sum(((np.eye(len(P_y)) - P_y) @ P_x) ** 2)) ** 0.5\n",
    "\n",
    "\n",
    "valRef, vecRef, DGrid_Reshape, L = MB_Model.Ref_Subspace(\n",
    "    MB_Model.V, k=3, beta=2, res=100, xrange=[-1.5, 1], yrange=[-0.5, 2], Deep=False\n",
    ")\n",
    "DGrid = DGrid_Reshape[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbbcafed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tau = 300\n",
    "tau_net = tau\n",
    "stride = 1\n",
    "N = 50000\n",
    "dt = 0.001\n",
    "beta = 2.0\n",
    "Deep = False  # We are using the easier Muller-Brown, in alignment with the tests in the paper.\n",
    "\n",
    "Xs = MB_Model.Sample_Starts(N, vmax=12)\n",
    "Integrator_Params = [dt, beta, stride, tau + 1, Deep]\n",
    "ps = []\n",
    "for x in Xs:\n",
    "    ps.append([x] + Integrator_Params)\n",
    "Trajs = MB_Model.Integrator(ps)\n",
    "Data = []\n",
    "for t in Trajs:\n",
    "    Data.append(np.asarray(t[0 : max(tau, tau_net) + 1]))\n",
    "\n",
    "# no need to use stopped trajectories since we don't have boundary conditions\n",
    "Data = np.asarray(Data)[:, :, 0:2]\n",
    "ntraj = Data.shape[0]\n",
    "lentraj = Data.shape[1]\n",
    "Data_tau = np.zeros((Data.shape[0], 2, Data.shape[-1]))\n",
    "Data_tau[:, 0] = Data[:, 0]\n",
    "Data_tau[:, -1] = Data[:, tau_net]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe850b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 3  # 3-D Subspace\n",
    "\n",
    "phi_t = Initialize(Data_tau, k)\n",
    "# only 2 functions are represented with the net.  The neural network will\n",
    "# concatenate the constant function as its first output, since we know that this is an eigenfunction.  Thus the\n",
    "# output of the net F=VPM.FF_Subspace(X,params) will have shape (X.shape[0],X.shape[1],k), with F[...,0]=1.\n",
    "layer_sizes = [\n",
    "    2,\n",
    "    64,\n",
    "    64,\n",
    "    64,\n",
    "    64,\n",
    "    64,\n",
    "    64,\n",
    "    k - 1,\n",
    "]\n",
    "\n",
    "params = VPM.init_network_params(layer_sizes, jax.random.PRNGKey(0), scale=0.01)\n",
    "params.append(np.zeros(k))\n",
    "params.append(np.eye(k))\n",
    "\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    params, phi_t, opt_state, R = VPM.Subspace_Iteration(\n",
    "        params,\n",
    "        Data_tau,\n",
    "        phi_t,\n",
    "        optimizer,\n",
    "        opt_state,\n",
    "        Inner_Iter=5000,\n",
    "        alpha=1 / ((i + 1) ** 0.5),\n",
    "        BS=2000,\n",
    "        Lossd=VPM.Loss_Subspaced,\n",
    "    )\n",
    "\n",
    "    val_t, vec_t = VPM.VAC(phi_t)\n",
    "    print(\"Eigenvalues= \")\n",
    "    print(val_t)\n",
    "    F = VPM.FF_Subspace(DGrid, params)\n",
    "    # we orthogonalize with the zero BC problem here, so we need to multiply by R^-1\n",
    "    Eiggrid = F @ np.linalg.inv(R) @ vec_t\n",
    "\n",
    "    sd = Subspace_Distance(\n",
    "        np.array(Eiggrid)[:, 0], vecRef\n",
    "    )  # The subspace distance is a measure of agreement between\n",
    "    # the nets and the grid-based reference. For a k-dimensional subspace it falls in the range(0,k)\n",
    "    print(\"Subspace Distance = \" + str(sd))\n",
    "    plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=Eiggrid[:, 0, 1], cmap=\"bwr\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.scatter(DGrid[:, 0, 0], DGrid[:, 0, 1], c=Eiggrid[:, 0, 2], cmap=\"bwr\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
